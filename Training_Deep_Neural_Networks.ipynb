{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwBcmJZE1mrflOMm1+kbAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarshp1/Learning_TensorFlow/blob/master/Training_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPfoLs4DZ8XW"
      },
      "source": [
        "\n",
        "References: Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow by Aurélien Géron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mwmVtjyY-Qd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_wDmkNJaIQW"
      },
      "source": [
        "## Glorot and He Initialization\n",
        "\n",
        "$fan_{in}$ is the number of inputs to the neuron. <br />\n",
        "$fan_{out}$ is the number of outputs of tht neuron. <br />\n",
        "$fan_{avg} = (fan_{in} + fan_{out})/2$\n",
        "\n",
        "Glorot initialization when using the logistic activation function:\n",
        "* Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
        "* Or a uniform distribution between $-r$ and $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$\n",
        "\n",
        "LeCun Initialization:\n",
        "Replace $fan_{avg}$ with $fan_{in}$ in Glorot Initialization. In other words the LeCun initialization is equivalent to the Glorot Initialization when $fan_{in}$ = $fan_{out}$. <br /> <br />\n",
        "\n",
        "|**Initialisation**| **Activation functions** | **$\\sigma^2$ (Normal)** |\n",
        "|--- |---| ---|\n",
        "|Glorot| None, tanh, logistic, softmax | $\\frac{1}{fan_{avg}}$ |\n",
        "| He | ReLU and its variants | $\\frac{2}{fan_{in}}$ |\n",
        "| LeCun | SeLU | $\\frac{1}{fan_{in}}$ |\n",
        "\n",
        "The table list only the variance for the normal initialisation. If you want to use the uniform distribution for the initialisation in the range $-r$ to $+r$, compute $r = \\sqrt {3\\sigma^2}$. \n",
        "\n",
        "By default, Keras uses Glorot initialisation with a uniform distribution. When creating a layer, you can change this to He initialisation by setting `kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\"` like this:\n",
        "```python\n",
        "    keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "```\n",
        "If you want the He initialization with a uniform distribution but based on $fan_{avg}$ rather than $fan_{in}$, you can use the `VariableScaling` initializer like this:\n",
        "\n",
        "```python\n",
        "    he_avg_init = keras.initializers.VariableScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
        "    keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQVG9W-6hDlM"
      },
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqLMC9FpaAl6",
        "outputId": "ee35ea13-a258-4817-8797-20f331e28793"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28,28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation='elu', kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8edIhDviuZr",
        "outputId": "c454d673-69ff-4d13-f79e-cdace6285d0e"
      },
      "source": [
        "# Parameters of the first BN Layers, Two trainable and two are not\n",
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization_1/gamma:0', True),\n",
              " ('batch_normalization_1/beta:0', True),\n",
              " ('batch_normalization_1/moving_mean:0', False),\n",
              " ('batch_normalization_1/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7es1F_zjbBj"
      },
      "source": [
        "The last two parameters **$\\mu$** and **$\\sigma$**, are the moving averages; they are not affected by backpropagation, so Keras calls them \"non-trainable\".\n",
        "\n",
        "Now when you create a BN layer in Keras, it also creates two operations that will be called by Keras at each iteration during training. These operations will update the moving averages, Since we are using the Tensorflow backend, these operations are TensorFlow operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NznAl22tjVTZ",
        "outputId": "b0d32daf-31d7-4307-d693-8e7139947e18"
      },
      "source": [
        "model.layers[2].updates"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1331: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqDBBdXbkkWk"
      },
      "source": [
        "The authors of the BN paper argued in favour of adding the BN layers before the activation functions. rather than after (as we just did). To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass `use_bias=False` when creating it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOdxusfgkTb0"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28,28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('elu')\n",
        "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('elu'),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj52uCLVlldx"
      },
      "source": [
        "**Hyperparameters of BN:**\n",
        "* `momentum`: This hyperparameter is used by the `BatchNormalization` layer when it updates the exponential moving averages; given a new value **$v$** (i.e., a new vector of input means or standard deviations computed over the current batch), the layer updates the running average **$\\hat{v}$** using the following equation:\n",
        "$$ \\hat{\\textbf{v}} \\leftarrow \\hat{\\textbf{v}} \\times momentum + \\textbf{v} \\times (1-momentum) $$\n",
        "A good momentum value is typically close to 1; for example. 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and smaller mini-batches). The defaults will usually be fine, but you may occasionally need to tweak.\n",
        "\n",
        "* `axis`: This hyperparameter decides which axis to normalize. It defaults to -1, meaning that by-default it will normalize the last axis (using the means and standard deviations computed across other axes). When the input batch is 2D (i.e., the batch shape is [*batch size*, *features*]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. If the input to the BN layer is 3D, with shape [*batch size*, *height*, *width*]; it will normalize across all the instances in the batch and across all the rows in the column. If you want to treat each of the element of a training example i.e. each element of the array in a training example as different, then you should set `axis=[1,2]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdMxBDxjqasY"
      },
      "source": [
        "Notice that the BN layer does not perform the same computation during training and after training: it uses batch statistics during training and the \"final\" statistics after training (i.e. the final values of the moving averages). The source code for this class looks like:\n",
        "```python\n",
        "    class BatchNormalization(keras.layers.Layer):\n",
        "        [...]\n",
        "        def call(self, inputs, training=None):\n",
        "            [...]\n",
        "```\n",
        "The `call()` method is the one that performs the computations; as you can see, it has an extra `training` argument, which is set to `None` by default, but the `fit()` method sets it to 1 during training.\n",
        "\n",
        "**TIP**: If you ever need to write a custom layerm and it must behave differently during training and testing, add a `training` argument to the `call()` method and use this argument in the method to decide what to compute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbOWoNKAqZvs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}